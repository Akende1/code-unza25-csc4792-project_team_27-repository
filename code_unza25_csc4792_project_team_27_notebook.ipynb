{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akende1/code-unza25-csc4792-project_team_27-repository/blob/main/code_unza25_csc4792_project_team_27_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYG27SW_8vcv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cr2hAk_dTqy"
      },
      "source": [
        "# 1.Business Understanding\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "Wikipedia serves as a widely accessible knowledge repository, however the quality and completeness of articles vary. For Zambian topics, many articles remain underdeveloped, lacking depth, proper structure, sufficient references, and multimedia elements. This limits the availability of reliable, comprehensive information about Zambia for both local and global audiences.\n",
        "\n",
        "Currently the process of identifying incomplete Zambian articles is largely manual, relying on editor intuition, quality assessments, and community discussions. This approach is time-consuming, inconsistent, and insufficient for  addressing the most critical content gaps. Without a scalable method to assess and prioritize articles, valuable editing efforts may be misdirected, leaving important topics underrepresented.\n",
        "\n",
        "\n",
        "The core problem is the absence of an automated, consistent, and accurate system to determine and classify the completeness of Zambian Wikipedia pages. By analyzing text structure, metadata, and content coverage, such a system could classify articles according to Wikipedia’s quality scale and highlight key areas for improvement. This would enable editors to focus on the most impactful updates, improve the overall quality of Zambian content, and ensure that readers have access to well-developed, trustworthy information.\n",
        "\n",
        "Solving this problem benefits multiple stakeholders: Wikipedia editors seeking guidance on where to contribute, WikiProject Zambia aiming to raise overall article standards, researchers and educators relying on accurate information, and the general public seeking a richer understanding of Zambia.\n",
        "\n",
        "## Objectives and success criteria\n",
        "\n",
        "### 1.1 Primary Objectives\n",
        "\n",
        "We will build a classification model for:\n",
        "\n",
        "**Objective 1: Article Classification System**\n",
        "Develop a system to automatically classify Zambian Wikipedia articles into established quality levels based on Wikipedia’s quality scale:  \n",
        "- **Stub (Level 0):** <500 words, minimal structure, few/no references.  \n",
        "- **Start (Level 1):** 500–1500 words, basic structure, some references.  \n",
        "- **C-Class (Level 2):** >1500 words, good structure, adequate references.  \n",
        "- **B-Class (Level 3):** Comprehensive coverage, good references, proper structure.  \n",
        "- **Good Article (Level 4):** Meets Wikipedia’s “good article” criteria—well-written, neutral, fully referenced.  \n",
        "- **Featured Article (Level 5):** Exemplary standard, exceptional writing, comprehensive coverage.\n",
        "\n",
        "**Objective 2: Content Gap Identification**  \n",
        "Identify missing or underdeveloped elements within articles categorized as:  \n",
        "- Structural gaps (missing sections)  \n",
        "- Content gaps (missing topics)  \n",
        "- Reference gaps (unreliable sources)  \n",
        "- Multimedia gaps (missing images, maps, diagrams)\n",
        "\n",
        "**Objective 3: Actionable Insights Generation**  \n",
        "Provide specific improvement recommendations, including:  \n",
        "- Priority ranking of articles needing attention  \n",
        "- Article-specific suggestions for enhancement  \n",
        "- Content templates for common article types  \n",
        "- Reference improvement strategies\n",
        "\n",
        "### 1.2 Secondary Objectives\n",
        "- Create a reusable assessment framework adaptable to other countries/topics.  \n",
        "- Establish baseline metrics to track future Wikipedia content improvements.  \n",
        "- Produce educational resources to guide editors on quality standards.\n",
        "\n",
        "## 2. Success Criteria & Metrics\n",
        "\n",
        "### 2.1 Primary Success Metrics\n",
        "**Metric 1: Classification Accuracy**  \n",
        "- **Target:** ≥85% accuracy in classifying articles.  \n",
        "- **Measurement:** Compare automated classifications against human-rated articles using cross-validation and confusion matrix analysis.\n",
        "\n",
        "**Metric 2: Gap Identification Accuracy**  \n",
        "- **Target:** Correctly flag the top 20 most incomplete Zambian articles.  \n",
        "- **Measurement:** Validate with expert review and compare with community consensus.\n",
        "\n",
        "### 2.2 Secondary Success Metrics\n",
        "**Model Performance Metrics**  \n",
        "- **Precision:** Minimize false positives (overrating article quality).  \n",
        "- **Recall:** Minimize false negatives (underrating article quality).  \n",
        "- **F1-Score:** Balanced metric for each quality level.  \n",
        "- **Cohen’s Kappa:** Measure agreement between model and human raters.\n",
        "\n",
        "**Business Impact Metrics**  \n",
        "- **Usability:** Wikipedia editors can easily interpret and act on recommendations.  \n",
        "- **Efficiency:** Reduced time needed to identify priority improvement opportunities.  \n",
        "- **Coverage:** Percentage of Zambian articles assessed by the system.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Validation Methods\n",
        "**Expert Review**  \n",
        "- Recruit 3–5 experienced Wikipedia editors.  \n",
        "- Have them manually assess 50 randomly selected articles.  \n",
        "- Compare their assessments with model output to measure agreement.\n",
        "\n",
        "**Community Feedback**  \n",
        "- Share results with WikiProject Zambia members.  \n",
        "- Gather qualitative feedback on usefulness, accuracy, and priorities.  \n",
        "- Apply feedback to refine the model and recommendations.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Data Mining Goals\n",
        "\n",
        "**Automated Quality Classification**\n",
        "\n",
        "Build a supervised classification model to automatically assign Zambian Wikipedia articles to one of six quality levels (Stub, Start, C-Class, B-Class, Good Article, Featured Article) based on Wikipedia’s quality scale.\n",
        "\n",
        "- Content Gap Detection\n",
        "\n",
        "Use text mining and metadata analysis to identify missing sections, low reference counts, insufficient word count, and lack of multimedia.\n",
        "\n",
        "- Improvement Recommendation Generation\n",
        "\n",
        "Develop a rule-based recommendation system that generates actionable suggestions for editors based on detected gaps (e.g., “Add references to support claims,” “Include an infobox,” “Expand the history section”).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usewYXf82Pz8"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# 2.Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWPXgI-97Uyg"
      },
      "outputs": [],
      "source": [
        "#Load CSV\n",
        "import pandas as pd\n",
        "df_raw = pd.read_csv('zambian_wikipedia_pages_dataset_FIXED.csv')\n",
        "df_raw.head()\n",
        "\n",
        "\n",
        "\n",
        "#Word count histogram\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(df_raw['word_count'], bins=20)\n",
        "plt.title('Word Count Distribution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Image count histogram\n",
        "plt.hist(df_raw['num_images'], bins=10)\n",
        "plt.title('Image Count Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tboD1JhR7xGV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M2RSPXG5oFa"
      },
      "source": [
        "# 3.Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPQca5wz9utF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Heuristic label function\n",
        "def heuristic_quality_label(row):\n",
        "    wc = row['word_count']; refs = row['references_signal']; secs = row['num_sections']\n",
        "    if wc<500 and refs<2 and secs<=2: return 0\n",
        "    if 500<=wc<1500 and refs>=2: return 1\n",
        "    if wc>=1500 and refs>=5 and secs>=5: return 2\n",
        "    if wc>=2000 and refs>=8 and secs>=6: return 3\n",
        "    if wc>=2500 and refs>=12 and secs>=8: return 4\n",
        "    if wc>=3000 and refs>=18 and secs>=10: return 5\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Features & scaling\n",
        "feature_cols = ['word_count','num_sections','num_internal_links','num_images','num_external_links','category_count','has_infobox','references_signal']\n",
        "X = df_raw[feature_cols]\n",
        "y = df_raw['quality_label']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = X.copy()\n",
        "X_scaled[feature_cols] = scaler.fit_transform(X[feature_cols])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Fq7zGOQ56Cj"
      },
      "source": [
        "# 4.Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfWIeMql-Hb6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
        "\n",
        "#Initialize models\n",
        "models = {\n",
        "    'LogReg': LogisticRegression(max_iter=200),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=250, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Feature importance for RandomForest\n",
        "rf = models['RandomForest']\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "feat_imp = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
        "feat_imp.plot(kind='bar', title='RandomForest Feature Importance')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#display confusion matrix\n",
        "preds = rf.predict(X_test)\n",
        "print(confusion_matrix(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJb7jMQS5_do"
      },
      "source": [
        "# 5.Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dtdk6sR4yVh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "#Select best model\n",
        "best_model = rf\n",
        "\n",
        "\n",
        "#Identify top misclassified articles\n",
        "y_pred = best_model.predict(X_scaled)\n",
        "misclassified = df_raw[y_pred != y]\n",
        "misclassified[['title','quality_label']].head(10)\n",
        "\n",
        "\n",
        "# confusion matrix heatmap\n",
        "import seaborn as sns\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZ6A96sCtT_"
      },
      "source": [
        "# 6.Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfQc-UPcC1sC"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "#Save trained RandomForest model\n",
        "joblib.dump(best_model, 'rf_quality_classifier.joblib')\n",
        "\n",
        "\n",
        "#export predictions\n",
        "df_raw.to_csv('zambian_wikipedia_pages_predictions.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
